{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import keras\n",
    "print keras.__version__ #version 2.1.2\n",
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n50EleReviews.json\\ny : size =  50000\\nX : size =  50000\\ntarget_names : size =  50\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = '50EleReviews.json' #origial review documents, there are 50 classes\n",
    "with open(fn, 'r') as infile:\n",
    "        docs = json.load(infile)\n",
    "X = docs['X']\n",
    "y = np.asarray(docs['y'])\n",
    "num_classes = len(docs['target_names'])\n",
    "\n",
    "'''\n",
    "50EleReviews.json\n",
    "y : size =  50000\n",
    "X : size =  50000\n",
    "target_names : size =  50\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words:  79259\n"
     ]
    }
   ],
   "source": [
    "#count each word's occurance\n",
    "def count_word(X):\n",
    "    word_count = dict()\n",
    "    for d in X:\n",
    "        for w in d.lower().split(' '): #lower\n",
    "            if w in word_count:\n",
    "                word_count[w] += 1\n",
    "            else:\n",
    "                word_count[w] = 1            \n",
    "    return word_count\n",
    "\n",
    "word_count = count_word(X)\n",
    "print 'total words: ', len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent word size =  12241\n"
     ]
    }
   ],
   "source": [
    "#get frequent words\n",
    "freq_words = [w  for w, c in word_count.iteritems() if c > 10]\n",
    "print 'frequent word size = ', len(freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word index\n",
    "word_to_idx = {w: i+2  for i, w in enumerate(freq_words)} # index 0 for padding, index 1 for unknown/rare words\n",
    "idx_to_word = {i:w for w, i in word_to_idx.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_word(X):\n",
    "    seqs = []\n",
    "    max_length = 0\n",
    "    for d in X:\n",
    "        seq = []\n",
    "        for w in d.lower().split():\n",
    "            if w in word_to_idx:\n",
    "                seq.append(word_to_idx[w])\n",
    "            else:\n",
    "                seq.append(1) #rare word index = 1\n",
    "        seqs.append(seq)\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index documents and pad each review to length = 3000\n",
    "indexed_X = index_word(X)\n",
    "padded_X = preprocessing.sequence.pad_sequences(indexed_X, maxlen=3000, dtype='int32', padding='post', truncating='post', value = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 3000) (35000,) (15000, 3000) (15000,)\n"
     ]
    }
   ],
   "source": [
    "#split review into training and testing set\n",
    "def splitTrainTest(X, y, ratio = 0.7): # 70% for training, 30% for testing\n",
    "    shuffle_idx = np.random.permutation(len(y))\n",
    "    split_idx = int(0.7*len(y))\n",
    "    shuffled_X = X[shuffle_idx]\n",
    "    shuffled_y = y[shuffle_idx]\n",
    "    \n",
    "    return shuffled_X[:split_idx], shuffled_y[:split_idx], shuffled_X[split_idx:], shuffled_y[split_idx:]   \n",
    "\n",
    "train_X, train_y, test_X, test_y = splitTrainTest(padded_X, y)\n",
    "\n",
    "print train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#split reviews into seen classes and unseen classes\n",
    "def splitSeenUnseen(X, y, seen, unseen):\n",
    "    seen_mask = np.in1d(y, seen)# find examples whose label is in seen classes\n",
    "    unseen_mask = np.in1d(y, unseen)# find examples whose label is in unseen classes\n",
    "    \n",
    "    print np.array_equal(np.logical_and(seen_mask, unseen_mask), np.zeros((y.shape), dtype= bool))#expect to see 'True', check two masks are exclusive\n",
    "    \n",
    "    # map elements in y to [0, ..., len(seen)] based on seen, map y to unseen_label when it belongs to unseen classes\n",
    "    to_seen = {l:i for i, l in enumerate(seen)}\n",
    "    unseen_label = len(seen)\n",
    "    to_unseen = {l:unseen_label for l in unseen}\n",
    "        \n",
    "    return X[seen_mask], np.vectorize(to_seen.get)(y[seen_mask]), X[unseen_mask], np.vectorize(to_unseen.get)(y[unseen_mask])\n",
    "\n",
    "seen = range(25)#seen classes\n",
    "unseen = range(25,50)#unseen classes\n",
    "\n",
    "seen_train_X, seen_train_y, _, _ = splitSeenUnseen(train_X, train_y, seen, unseen)\n",
    "seen_test_X, seen_test_y, unseen_test_X, unseen_test_y = splitSeenUnseen(test_X, test_y, seen, unseen)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "cate_seen_train_y = to_categorical(seen_train_y, len(seen))#make train y to categorial/one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network, in the paper, I use pretrained google news embedding, here I do not use it and set the embedding layer trainable\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding, Input, Concatenate\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "def Network(MAX_SEQUENCE_LENGTH = 3000, EMBEDDING_DIM = 300, nb_word = len(word_to_idx)+2, filter_lengths = [3, 4, 5],\n",
    "    nb_filter = 150, hidden_dims =250):\n",
    "    \n",
    "    graph_in = Input(shape=(MAX_SEQUENCE_LENGTH,  EMBEDDING_DIM))\n",
    "    convs = []\n",
    "    for fsz in filter_lengths:\n",
    "        conv = Conv1D(filters=nb_filter,\n",
    "                                 kernel_size=fsz,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu')(graph_in)\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        convs.append(pool)\n",
    "\n",
    "    if len(filter_lengths)>1:\n",
    "        out = Concatenate(axis=-1)(convs)\n",
    "    else:\n",
    "        out = convs[0]\n",
    "\n",
    "    graph = Model(inputs=graph_in, outputs=out) #convolution layers\n",
    "    \n",
    "    emb_layer = [Embedding(nb_word,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True),\n",
    "                 Dropout(0.2)\n",
    "        ]\n",
    "    conv_layer = [\n",
    "            graph,\n",
    "        ]\n",
    "    feature_layers1 = [\n",
    "            Dense(hidden_dims),\n",
    "            Dropout(0.2),\n",
    "            Activation('relu')\n",
    "    ]\n",
    "    feature_layers2 = [\n",
    "            Dense(len(seen) ),\n",
    "            Dropout(0.2),\n",
    "    ]\n",
    "    output_layer = [\n",
    "            Activation('sigmoid')\n",
    "    ]\n",
    "\n",
    "    model = Sequential(emb_layer+conv_layer+feature_layers1+feature_layers2+output_layer)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3000, 300)         3672900   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3000, 300)         0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 450)               540450    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               112750    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                6275      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 25)                0         \n",
      "=================================================================\n",
      "Total params: 4,332,375\n",
      "Trainable params: 4,332,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Network()    \n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huxu/anaconda2/lib/python2.7/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14045 samples, validate on 3512 samples\n",
      "Epoch 1/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9537Epoch 00001: val_loss improved from inf to 0.14751, saving model to bestmodel.h5\n",
      "14045/14045 [==============================] - 75s 5ms/step - loss: 0.2972 - acc: 0.9537 - val_loss: 0.1475 - val_acc: 0.9652\n",
      "Epoch 2/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9705Epoch 00002: val_loss improved from 0.14751 to 0.07313, saving model to bestmodel.h5\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.2098 - acc: 0.9705 - val_loss: 0.0731 - val_acc: 0.9800\n",
      "Epoch 3/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9773Epoch 00003: val_loss improved from 0.07313 to 0.05967, saving model to bestmodel.h5\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1846 - acc: 0.9773 - val_loss: 0.0597 - val_acc: 0.9825\n",
      "Epoch 4/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9801Epoch 00004: val_loss improved from 0.05967 to 0.05341, saving model to bestmodel.h5\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1739 - acc: 0.9801 - val_loss: 0.0534 - val_acc: 0.9842\n",
      "Epoch 5/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9827Epoch 00005: val_loss improved from 0.05341 to 0.04974, saving model to bestmodel.h5\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1654 - acc: 0.9827 - val_loss: 0.0497 - val_acc: 0.9848\n",
      "Epoch 6/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9848Epoch 00006: val_loss improved from 0.04974 to 0.04631, saving model to bestmodel.h5\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1598 - acc: 0.9848 - val_loss: 0.0463 - val_acc: 0.9855\n",
      "Epoch 7/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9871Epoch 00007: val_loss improved from 0.04631 to 0.04568, saving model to bestmodel.h5\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1525 - acc: 0.9871 - val_loss: 0.0457 - val_acc: 0.9854\n",
      "Epoch 8/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9889Epoch 00008: val_loss did not improve\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1485 - acc: 0.9889 - val_loss: 0.0459 - val_acc: 0.9855\n",
      "Epoch 9/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9901Epoch 00009: val_loss did not improve\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1457 - acc: 0.9902 - val_loss: 0.0471 - val_acc: 0.9851\n",
      "Epoch 10/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9904Epoch 00010: val_loss did not improve\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1440 - acc: 0.9904 - val_loss: 0.0491 - val_acc: 0.9850\n",
      "Epoch 11/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9913Epoch 00011: val_loss did not improve\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1426 - acc: 0.9913 - val_loss: 0.0505 - val_acc: 0.9853\n",
      "Epoch 12/100\n",
      "13952/14045 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9913Epoch 00012: val_loss did not improve\n",
      "14045/14045 [==============================] - 70s 5ms/step - loss: 0.1419 - acc: 0.9913 - val_loss: 0.0527 - val_acc: 0.9852\n"
     ]
    }
   ],
   "source": [
    "bestmodel_path = 'bestmodel.h5'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=bestmodel_path, verbose=1, save_best_only=True)\n",
    "early_stopping=EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(seen_train_X, cate_seen_train_y,\n",
    "              epochs=100, batch_size=128, callbacks=[checkpointer, early_stopping], validation_split=0.2)\n",
    "\n",
    "model.load_weights(bestmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on training examples for cauculate standard deviation\n",
    "seen_train_X_pred = model.predict(seen_train_X)\n",
    "print seen_train_X_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a gaussian model\n",
    "from scipy.stats import norm as dist_model\n",
    "def fit(prob_pos_X):\n",
    "    prob_pos = [p for p in prob_pos_X]+[2-p for p in prob_pos_X]\n",
    "    pos_mu, pos_std = dist_model.fit(prob_pos)\n",
    "    return pos_mu, pos_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.22340473697969346], [1.0, 0.20191789082475273], [1.0, 0.21697574180383722], [1.0, 0.27814359593764837], [1.0, 0.20906240683374575], [1.0, 0.2120679142006848], [1.0, 0.12389883676772227], [1.0, 0.31187556613282014], [1.0, 0.36422199393287763], [1.0, 0.26286166628319735], [1.0, 0.31564927985085572], [1.0, 0.17614791363233634], [1.0, 0.37331221261718889], [1.0, 0.36484709145498223], [1.0, 0.26390320190354633], [1.0, 0.30076693332062482], [1.0, 0.26889371122555278], [1.0, 0.086930575088141823], [1.0, 0.17107333902941718], [1.0, 0.24341024513612153], [1.0, 0.16968459774298794], [1.0, 0.18485411008479252], [1.0, 0.13531653771043087], [1.0, 0.13280404404602544], [1.0, 0.33530382577148005]]\n"
     ]
    }
   ],
   "source": [
    "#calculate mu, std of each seen class\n",
    "mu_stds = []\n",
    "for i in range(len(seen)):\n",
    "    pos_mu, pos_std = fit(seen_train_X_pred[seen_train_y==i, i])\n",
    "    mu_stds.append([pos_mu, pos_std])\n",
    "\n",
    "print mu_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 25) (15000,)\n"
     ]
    }
   ],
   "source": [
    "#predict on test examples\n",
    "test_X_pred = model.predict(np.concatenate([seen_test_X,unseen_test_X], axis = 0))\n",
    "test_y_gt = np.concatenate([seen_test_y,unseen_test_y], axis = 0)\n",
    "print test_X_pred.shape, test_y_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get prediction based on threshold\n",
    "test_y_pred = []\n",
    "scale = 1.\n",
    "for p in test_X_pred:# loop every test prediction\n",
    "    max_class = np.argmax(p)# predicted class\n",
    "    max_value = np.max(p)# predicted probability\n",
    "    threshold = max(0.5, 1. - scale * mu_stds[max_class][1])#find threshold for the predicted class\n",
    "    if max_value > threshold:\n",
    "        test_y_pred.append(max_class)#predicted probability is greater than threshold, accept\n",
    "    else:\n",
    "        test_y_pred.append(len(seen))#otherwise, reject\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro fscore:  0.672538640119\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, _ = precision_recall_fscore_support(test_y_gt, test_y_pred)\n",
    "print 'macro fscore: ', np.mean(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
